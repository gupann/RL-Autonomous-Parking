{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupann/CS269-Parking/blob/anmol-parallel-parking-static-obstacles-env/parallelparking_model_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eeje4O8fviH",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Model-Based Reinforcement Learning\n",
        "\n",
        "## Parallel Parking\n",
        "parking-parallel-dynObs-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzMSuJEOfviP",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Models and computation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# deleting old videos\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "folder_path = r\"/home/aayush_wsl/cs269_rl_parking/CS269-Parking/videos\"\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(\"Video Folder deleted.\")\n",
        "else:\n",
        "    print(\"Video Folder does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Bu_Pqop0E7"
      },
      "source": [
        "We also define a simple helper function for visualization of episodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "so7yH4ucyB-3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from tqdm import trange # from tqdm.notebook import trange\n",
        "# !pip install tensorboardx gym pyvirtualdisplay\n",
        "# !apt-get install -y xvfb ffmpeg\n",
        "# !git clone https://github.com/gupann/CS269-Parking.git 2> /dev/null\n",
        "sys.path.insert(0, '/home/aayush_wsl/cs269_rl_parking/CS269-Parking/scripts')\n",
        "from utils import record_videos, show_videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm-hQ06B8pNr"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "\n",
        "gym.register_envs(highway_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFtBY6JSqPFa"
      },
      "source": [
        "### Let's try it!\n",
        "\n",
        "Make the environment, and run an episode with random actions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkkAmSOo-nd3"
      },
      "source": [
        "Parallel Parking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for parallel parking with custom dimensions\n",
        "import numpy as np\n",
        "\n",
        "parking_config = {\n",
        "    # ===== PARKING SLOT DIMENSIONS =====\n",
        "    \"street_length\": 80.0,      # Total length of street (default: 60.0)\n",
        "                                # Increasing this makes parking spots wider\n",
        "    \n",
        "    \"n_slots\": 6,               # Number of slots per side (default: 8)\n",
        "                                # Decreasing this makes parking spots wider\n",
        "    \n",
        "    # NOTE: Parking slot width = (street_length - 2*5.0) / n_slots\n",
        "    # With above values: (80 - 10) / 6 = 11.67m per slot\n",
        "    # Default: (60 - 10) / 8 = 6.25m per slot\n",
        "    \n",
        "    # ===== LANE DIMENSIONS =====\n",
        "    \"lane_width\": 30.0,         # Width of central driving lane (default: 10.0)\n",
        "                                # This is the vertical space for ego to maneuver\n",
        "    \n",
        "    \"curb_offset\": 15.0,        # Distance from center to parking rows (default: 10.0)\n",
        "                                # Increase this to move parking spots farther from center\n",
        "    \n",
        "    # ===== GOAL/STARTING POSITIONS =====\n",
        "    \"empty_slot_index\": 3,      # Which bottom slot is empty (0-based)\n",
        "    \n",
        "    # ===== ENVIRONMENT BOUNDARIES =====\n",
        "    \"wall_margin\": 4.0,         # Extra space outside parking area\n",
        "    \"add_walls\": True,          # Yellow boundary walls\n",
        "    \n",
        "    # ===== SIMULATION PARAMETERS =====\n",
        "    \"observation\": {\n",
        "        \"type\": \"KinematicsGoal\",\n",
        "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
        "        \"scales\": [100, 100, 5, 5, 1, 1],\n",
        "        \"normalize\": False,\n",
        "    },\n",
        "    \"action\": {\"type\": \"ContinuousAction\"},\n",
        "    \"reward_weights\": [1, 1, 0, 0, 0.2, 0.2],\n",
        "    \"success_goal_reward\": 0.10, #Changed from 0.12 to 0.10 (Tighter)\n",
        "    \"collision_reward\": -5,\n",
        "    \"steering_range\": np.deg2rad(45),\n",
        "    \"simulation_frequency\": 15,\n",
        "    \"policy_frequency\": 5,\n",
        "    \"duration\": 100,\n",
        "    \"screen_width\": 800,        # Wider screen for wider environment\n",
        "    \"screen_height\": 400,       # Taller screen for wider lane\n",
        "    \"centering_position\": [0.5, 0.5],\n",
        "    \"scaling\": 5,               # Adjust zoom level (lower = more zoomed out)\n",
        "    \"show_trajectories\": False,\n",
        "    \"controlled_vehicles\": 1,\n",
        "    \"offscreen_rendering\": False,\n",
        "    \"manual_control\": False,\n",
        "    \"real_time_rendering\": False,\n",
        "}\n",
        "\n",
        "# Calculate and display the resulting dimensions\n",
        "slot_width = (parking_config[\"street_length\"] - 10.0) / parking_config[\"n_slots\"]\n",
        "print(f\"üìè Configuration Summary:\")\n",
        "print(f\"  ‚Ä¢ Driving lane width: {parking_config['lane_width']} m\")\n",
        "print(f\"  ‚Ä¢ Parking slot width: {slot_width:.2f} m\")\n",
        "print(f\"  ‚Ä¢ Number of slots per row: {parking_config['n_slots']}\")\n",
        "print(f\"  ‚Ä¢ Total street length: {parking_config['street_length']} m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the dynamic obstacle environment\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "\n",
        "gym.register_envs(highway_env)\n",
        "\n",
        "# Create environment\n",
        "env = gym.make(\"parking-parallel-dynObs-v0\", render_mode=\"rgb_array\", config=parking_config)\n",
        "env = record_videos(env)\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Run a short episode to see the moving obstacle\n",
        "for step in range(100):\n",
        "    action = env.action_space.sample()  # Random action for testing\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    \n",
        "    # Print positions every 20 steps\n",
        "    if step % 20 == 0:\n",
        "        ego_pos = env.unwrapped.vehicle.position\n",
        "        moving_obs = env.unwrapped.road.vehicles[1]  # The moving obstacle\n",
        "        print(f\"Step {step}:\")\n",
        "        print(f\"  Ego position: {ego_pos}\")\n",
        "        print(f\"  Moving obstacle position: {moving_obs.position}\")\n",
        "        print(f\"  Moving obstacle speed: {moving_obs.speed} m/s\")\n",
        "    \n",
        "    if done or truncated:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "show_videos()\n",
        "print(\"\\n‚úÖ Dynamic obstacle is working!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H-GkGYTz-liD"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"parking-parallel-dynObs-v0\", render_mode=\"rgb_array\", config=parking_config)\n",
        "env = record_videos(env)\n",
        "env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "env.close()\n",
        "show_videos()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Studying the environment, action space, reward  \n",
        "1.) How and what modifications can be made to the environment  \n",
        "2.) Figuring out how the action space looks like  \n",
        "3.) How can I change the reward definition  \n",
        "4.) How is collision detected in the environment  \n",
        "5.) How is goal position specified in the environment  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(env.unwrapped.config[\"observation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewG5f_essAS0"
      },
      "source": [
        "The environment is a `GoalEnv`, which means the agent receives a dictionary containing both the current `observation` and the `desired_goal` that conditions its policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M3pmc1_-tWU"
      },
      "outputs": [],
      "source": [
        "print(\"Observation format:\", obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voagCILztJ3J"
      },
      "source": [
        "There is also an `achieved_goal` that won't be useful here (it only serves when the state and goal spaces are different, as a projection from the observation to the goal space).\n",
        "\n",
        "Alright! We are now ready to apply the model-based reinforcement learning paradigm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2PuVAvyfvib",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Experience collection\n",
        "First, we randomly interact with the environment to produce a batch of experiences\n",
        "\n",
        "$$D = \\{s_t, a_t, s_{t+1}\\}_{t\\in[1,N]}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UiUVp7tioyF",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ['state', 'action', 'next_state'])\n",
        "\n",
        "def collect_interaction_data(env, size=1000, action_repeat=2):\n",
        "    data, done = [], True\n",
        "    for _ in trange(size, desc=\"Collecting interaction data\"):\n",
        "        action = env.action_space.sample()\n",
        "        for _ in range(action_repeat):\n",
        "            if done:\n",
        "              previous_obs, info = env.reset()\n",
        "            obs, reward, done, truncated, info = env.step(action)\n",
        "            data.append(Transition(torch.Tensor(previous_obs[\"observation\"]),\n",
        "                                   torch.Tensor(action),\n",
        "                                   torch.Tensor(obs[\"observation\"])))\n",
        "            previous_obs = obs\n",
        "    return data\n",
        "\n",
        "env = gym.make(\"parking-parallel-dynObs-v0\", render_mode=\"rgb_array\", config=parking_config)\n",
        "data = collect_interaction_data(env)\n",
        "print(\"Sample transition:\", data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Th1JezEfvir"
      },
      "source": [
        "## Build a dynamics model\n",
        "\n",
        "We now design a model to represent the system dynamics. We choose  a **structured model** inspired from *Linear Time-Invariant (LTI) systems*\n",
        "\n",
        "$$\\dot{x} = f_\\theta(x, u) = A_\\theta(x, u)x + B_\\theta(x, u)u$$\n",
        "\n",
        "where the $(x, u)$ notation comes from the Control Theory community and stands for the state and action $(s,a)$. Intuitively, we learn at each point $(x_t, u_t)$ the **linearization** of the true dynamics $f$ with respect to $(x, u)$.\n",
        "\n",
        "We parametrize $A_\\theta$ and $B_\\theta$ as two fully-connected networks with one hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7Gl2kKJfviu",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class DynamicsModel(nn.Module):\n",
        "    STATE_X = 0\n",
        "    STATE_Y = 1\n",
        "\n",
        "    def __init__(self, state_size, action_size, hidden_size, dt):\n",
        "        super().__init__()\n",
        "        self.state_size, self.action_size, self.dt = state_size, action_size, dt\n",
        "        A_size, B_size = state_size * state_size, state_size * action_size\n",
        "        self.A1 = nn.Linear(state_size + action_size, hidden_size)\n",
        "        self.A2 = nn.Linear(hidden_size, A_size)\n",
        "        self.B1 = nn.Linear(state_size + action_size, hidden_size)\n",
        "        self.B2 = nn.Linear(hidden_size, B_size)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        \"\"\"\n",
        "            Predict x_{t+1} = f(x_t, u_t)\n",
        "        :param x: a batch of states\n",
        "        :param u: a batch of actions\n",
        "        \"\"\"\n",
        "        xu = torch.cat((x, u), -1)\n",
        "        xu[:, self.STATE_X:self.STATE_Y+1] = 0  # Remove dependency in (x,y)\n",
        "        A = self.A2(F.relu(self.A1(xu)))\n",
        "        A = torch.reshape(A, (x.shape[0], self.state_size, self.state_size))\n",
        "        B = self.B2(F.relu(self.B1(xu)))\n",
        "        B = torch.reshape(B, (x.shape[0], self.state_size, self.action_size))\n",
        "        dx = A @ x.unsqueeze(-1) + B @ u.unsqueeze(-1)\n",
        "        return x + dx.squeeze()*self.dt\n",
        "\n",
        "\n",
        "dynamics = DynamicsModel(state_size=env.observation_space.spaces[\"observation\"].shape[0],\n",
        "                         action_size=env.action_space.shape[0],\n",
        "                         hidden_size=64,\n",
        "                         dt=1/env.unwrapped.config[\"policy_frequency\"])\n",
        "print(\"Forward initial model on a sample transition:\",\n",
        "      dynamics(data[0].state.unsqueeze(0), data[0].action.unsqueeze(0)).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFsgc7gffvi0"
      },
      "source": [
        "## Fit the model on data\n",
        "We can now train our model $f_\\theta$ in a supervised fashion to minimize an MSE loss $L^2(f_\\theta; D)$ over our experience batch $D$ by stochastic gradient descent:\n",
        "\n",
        "$$L^2(f_\\theta; D) = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||s_{t+1}- f_\\theta(s_t, a_t)||^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwCDLD1wfvi2",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(dynamics.parameters(), lr=0.01)\n",
        "\n",
        "# Split dataset into training and validation\n",
        "train_ratio = 0.7\n",
        "train_data, validation_data = data[:int(train_ratio * len(data))], data[int(train_ratio * len(data)):]\n",
        "\n",
        "def compute_loss(model, data_t, loss_func = torch.nn.MSELoss()):\n",
        "    states, actions, next_states = data_t\n",
        "    predictions = model(states, actions)\n",
        "    return loss_func(predictions, next_states)\n",
        "\n",
        "def transpose_batch(batch):\n",
        "    return Transition(*map(torch.stack, zip(*batch)))\n",
        "\n",
        "def train(model, train_data, validation_data, epochs=1500):\n",
        "    train_data_t = transpose_batch(train_data)\n",
        "    validation_data_t = transpose_batch(validation_data)\n",
        "    losses = np.full((epochs, 2), np.nan)\n",
        "    for epoch in trange(epochs, desc=\"Train dynamics\"):\n",
        "        # Compute loss gradient and step optimizer\n",
        "        loss = compute_loss(model, train_data_t)\n",
        "        validation_loss = compute_loss(model, validation_data_t)\n",
        "        losses[epoch] = [loss.detach().numpy(), validation_loss.detach().numpy()]\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Plot losses\n",
        "    plt.plot(losses)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend([\"train\", \"validation\"])\n",
        "    plt.show()\n",
        "\n",
        "train(dynamics, data, validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXBODCuYfvi_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Visualize trained dynamics\n",
        "\n",
        "In order to qualitatively evaluate our model, we can choose some values of steering angle *(right, center, left)* and acceleration *(slow, fast)* in order to predict and visualize the corresponding trajectories from an initial state.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMPA55bCfvjB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def predict_trajectory(state, actions, model, action_repeat=1):\n",
        "    states = []\n",
        "    for action in actions:\n",
        "        for _ in range(action_repeat):\n",
        "            state = model(state, action)\n",
        "            states.append(state)\n",
        "    return torch.stack(states, dim=0)\n",
        "\n",
        "def plot_trajectory(states, color):\n",
        "    scales = np.array(env.unwrapped.config[\"observation\"][\"scales\"])\n",
        "    states = np.clip(states.squeeze(1).detach().numpy() * scales, -100, 100)\n",
        "    plt.plot(states[:, 0], states[:, 1], color=color, marker='.')\n",
        "    plt.arrow(states[-1,0], states[-1,1], states[-1,4]*1, states[-1,5]*1, color=color)\n",
        "\n",
        "def visualize_trajectories(model, state, horizon=15):\n",
        "    plt.cla()\n",
        "    # Draw a car\n",
        "    plt.plot(state.numpy()[0]+2.5*np.array([-1, -1, 1, 1, -1]),\n",
        "             state.numpy()[1]+1.0*np.array([-1, 1, 1, -1, -1]), 'k')\n",
        "    # Draw trajectories\n",
        "    state = state.unsqueeze(0)\n",
        "    colors = iter(plt.get_cmap(\"tab20\").colors)\n",
        "    # Generate commands\n",
        "    for steering in np.linspace(-0.5, 0.5, 3):\n",
        "        for acceleration in np.linspace(0.8, 0.4, 2):\n",
        "            actions = torch.Tensor([acceleration, steering]).view(1,1,-1)\n",
        "            # Predict trajectories\n",
        "            states = predict_trajectory(state, actions, model, action_repeat=horizon)\n",
        "            plot_trajectory(states, color=next(colors))\n",
        "    plt.axis(\"equal\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_trajectories(dynamics, state=torch.Tensor([0, 0, 0, 0, 1, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOa0j1_muNXi"
      },
      "source": [
        "## Reward model\n",
        "We assume that the reward $R(s,a)$ is known (chosen by the system designer), and takes the form of a **weighted L1-norm** between the state and the goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRubRv9buNXj"
      },
      "outputs": [],
      "source": [
        "def reward_model(states, goal, gamma=None):\n",
        "    \"\"\"\n",
        "        The reward is a weighted L1-norm between the state and a goal\n",
        "    :param Tensor states: a batch of states. shape: [batch_size, state_size].\n",
        "    :param Tensor goal: a goal state. shape: [state_size].\n",
        "    :param float gamma: a discount factor\n",
        "    \"\"\"\n",
        "    goal = goal.expand(states.shape)\n",
        "    reward_weigths = torch.Tensor(env.unwrapped.config[\"reward_weights\"])\n",
        "    rewards = -torch.pow(torch.norm((states-goal)*reward_weigths, p=1, dim=-1), 0.5)\n",
        "    if gamma:\n",
        "        time = torch.arange(rewards.shape[0], dtype=torch.float).unsqueeze(-1).expand(rewards.shape)\n",
        "        rewards *= torch.pow(gamma, time)\n",
        "    return rewards\n",
        "\n",
        "obs, info = env.reset()\n",
        "print(\"Reward of a sample transition:\", reward_model(torch.Tensor(obs[\"observation\"]).unsqueeze(0),\n",
        "                                                     torch.Tensor(obs[\"desired_goal\"])))\n",
        "print(obs[\"desired_goal\"])                                                     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5D6W4p7fvjI"
      },
      "source": [
        "## Leverage dynamics model for planning\n",
        "\n",
        "We now use the learnt dynamics model $f_\\theta$ for planning.\n",
        "In order to solve the optimal control problem, we use a sampling-based optimization algorithm: the **Cross-Entropy Method** (`CEM`). It is an optimization algorithm applicable to problems that are both **combinatorial** and **continuous**, which is our case: find the best performing sequence of actions.\n",
        "\n",
        "This method approximates the optimal importance sampling estimator by repeating two phases:\n",
        "1. **Draw samples** from a probability distribution. We use Gaussian distributions over sequences of actions.\n",
        "2. Minimize the **cross-entropy** between this distribution and a **target distribution** to produce a better sample in the next iteration. We define this target distribution by selecting the top-k performing sampled sequences.\n",
        "\n",
        "![Credits to Olivier Sigaud](https://github.com/yfletberliac/rlss2019-hands-on/blob/master/imgs/cem.png?raw=1)\n",
        "\n",
        "Note that as we have a local linear dynamics model, we could instead choose an `Iterative LQR` planner which would be more efficient. We prefer `CEM` in this educational setting for its simplicity and generality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzPKYg23fvjL",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def cem_planner(state, goal, action_size, horizon=5, population=100, selection=10, iterations=5):\n",
        "    state = state.expand(population, -1)\n",
        "    action_mean = torch.zeros(horizon, 1, action_size)\n",
        "    action_std = torch.ones(horizon, 1, action_size)\n",
        "    for _ in range(iterations):\n",
        "        # 1. Draw sample sequences of actions from a normal distribution\n",
        "        actions = torch.normal(mean=action_mean.repeat(1, population, 1), std=action_std.repeat(1, population, 1))\n",
        "        actions = torch.clamp(actions, min=env.action_space.low.min(), max=env.action_space.high.max())\n",
        "        states = predict_trajectory(state, actions, dynamics, action_repeat=5)\n",
        "        # 2. Fit the distribution to the top-k performing sequences\n",
        "        returns = reward_model(states, goal).sum(dim=0)\n",
        "        _, best = returns.topk(selection, largest=True, sorted=False)\n",
        "        best_actions = actions[:, best, :]\n",
        "        action_mean = best_actions.mean(dim=1, keepdim=True)\n",
        "        action_std = best_actions.std(dim=1, unbiased=False, keepdim=True)\n",
        "    return action_mean[0].squeeze(dim=0)\n",
        "\n",
        "\n",
        "# Run the planner on a sample transition\n",
        "action = cem_planner(torch.Tensor(obs[\"observation\"]),\n",
        "                     torch.Tensor(obs[\"desired_goal\"]),\n",
        "                     env.action_space.shape[0])\n",
        "print(\"Planned action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Diagnostic Script  \n",
        "1.) To address issues like non-moving ego vehicle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DIAGNOSTIC CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Check if model was trained\n",
        "print(\"\\n1. Model parameters (first few):\")\n",
        "print(list(dynamics.parameters())[0][0, :5])\n",
        "# If all zeros or very small, model not trained!\n",
        "\n",
        "# 2. Check action space\n",
        "print(\"\\n2. Action space:\")\n",
        "print(f\"   Low: {env.action_space.low}\")\n",
        "print(f\"   High: {env.action_space.high}\")\n",
        "\n",
        "# 3. Test model prediction\n",
        "obs, info = env.reset()\n",
        "test_action = torch.Tensor([0.5, 0.2])\n",
        "current = torch.Tensor(obs[\"observation\"])\n",
        "predicted = dynamics(current.unsqueeze(0), test_action.unsqueeze(0))\n",
        "print(\"\\n3. Model prediction test:\")\n",
        "print(f\"   Current position: {current[:2].numpy()}\")\n",
        "print(f\"   Predicted position: {predicted.squeeze()[:2].detach().numpy()}\")\n",
        "print(f\"   Position change: {(predicted.squeeze()[:2] - current[:2]).detach().numpy()}\")\n",
        "\n",
        "# 4. Test CEM planner\n",
        "print(\"\\n4. CEM planner test:\")\n",
        "action = cem_planner(current, torch.Tensor(obs[\"desired_goal\"]), \n",
        "                     env.action_space.shape[0])\n",
        "print(f\"   Planned action: {action.numpy()}\")\n",
        "print(f\"   Action magnitude: {torch.norm(action).item()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(env.unwrapped.config[\"reward_weights\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8L6vEPWyea7"
      },
      "source": [
        "## Visualize a few episodes\n",
        "\n",
        "En voiture, Simone !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOcOP7Of18T2"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"parking-parallel-dynObs-v0\", render_mode='rgb_array', config=parking_config)\n",
        "env = record_videos(env)\n",
        "obs, info = env.reset()\n",
        "\n",
        "for step in trange(3*env.unwrapped.config[\"duration\"], desc=\"Testing 3 episodes...\"):\n",
        "    action = cem_planner(torch.Tensor(obs[\"observation\"]),\n",
        "                         torch.Tensor(obs[\"desired_goal\"]),\n",
        "                         env.action_space.shape[0])\n",
        "    obs, reward, done, truncated, info = env.step(action.numpy())\n",
        "    if done or truncated:\n",
        "        obs, info = env.reset()\n",
        "env.close()\n",
        "show_videos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Desired Goal:  [ 0.45833333 -0.15        0.          0.          1.          0.        ]\n",
            "Observed Goal:  [0.22202574 0.04363642]\n",
            "Action Taken:  tensor([0.7805, 0.9334])\n",
            "Reward:  -0.8069012728880963\n",
            "Done:  False\n",
            "Truncated:  False\n",
            "Info:  {'speed': 7.840243625640871, 'crashed': False, 'action': array([0.7805107, 0.9333933], dtype=float32), 'is_success': np.False_}\n"
          ]
        }
      ],
      "source": [
        "#Analysis of the results\n",
        "print('Desired Goal: ', obs['desired_goal'])\n",
        "print('Observed Goal: ', obs['observation'][:2])\n",
        "print('Action Taken: ', action)\n",
        "print('Reward: ', reward)\n",
        "print('Done: ', done)\n",
        "print('Truncated: ', truncated)\n",
        "print('Info: ', info)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "parallelparking_model_based.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "highway-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
